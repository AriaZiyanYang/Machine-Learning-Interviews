{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb96ec95",
   "metadata": {},
   "source": [
    "### 1. 简介\n",
    "\n",
    "- 输入数据是 `(x, y⁺, y⁻)`：同一个 prompt，两个回答，一个好一个坏。\n",
    "- 目标：让 policy 模型在好回答上的 log-prob 高于坏回答。\n",
    "- 和 RLHF 不同：DPO 不需要 reward model，只需要偏好对。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b42d350",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def dpo_loss(policy_logp_pos, policy_logp_neg,\n",
    "             ref_logp_pos=None, ref_logp_neg=None,\n",
    "             beta=0.1, reference_free=False):\n",
    "    \"\"\"\n",
    "    Direct Preference Optimization (DPO) 损失函数\n",
    "    ------------------------------------------------\n",
    "    Args:\n",
    "        policy_logp_pos: policy 模型在 \"好回答 y⁺\" 上的 log-prob (batch,)\n",
    "        policy_logp_neg: policy 模型在 \"坏回答 y⁻\" 上的 log-prob (batch,)\n",
    "        ref_logp_pos:    reference 模型在 \"好回答 y⁺\" 上的 log-prob (batch,)\n",
    "        ref_logp_neg:    reference 模型在 \"坏回答 y⁻\" 上的 log-prob (batch,)\n",
    "        beta:            温度系数 (常取 0.05 ~ 0.5)，控制损失的平滑程度\n",
    "        reference_free:  若 True，则忽略 reference 模型（退化为无参对比学习）\n",
    "\n",
    "    Returns:\n",
    "        loss: 标量张量，batch 的平均 DPO 损失\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. policy 模型的 log-ratio：好回答 vs 坏回答\n",
    "    #    log πθ(y⁺|x) - log πθ(y⁻|x)\n",
    "    pi_logratio = policy_logp_pos - policy_logp_neg\n",
    "\n",
    "    # 2. reference 模型的 log-ratio（如果不开启 reference_free）\n",
    "    #    log π_ref(y⁺|x) - log π_ref(y⁻|x)\n",
    "    ref_logratio = 0 if reference_free else (ref_logp_pos - ref_logp_neg)\n",
    "\n",
    "    # 3. 最终对比 logits = policy 相对优势 - reference 相对优势\n",
    "    #    这是 DPO 论文里的 h_{πθ}^{y⁺, y⁻}\n",
    "    logits = pi_logratio - ref_logratio\n",
    "\n",
    "    # 4. DPO 损失\n",
    "    #    -log σ(β * logits)\n",
    "    #    当 logits 越大（说明 policy 更倾向好回答），loss 越小\n",
    "    loss = -F.logsigmoid(beta * logits).mean()\n",
    "\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05d3716d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Case 1 (reference_free=True) loss: 0.6468777656555176\n",
      "Case 2 (with reference, similar preference) loss: 0.6685033440589905\n",
      "Case 3 (with reference, opposite preference) loss: 0.5655654072761536\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# Case 0: 基础设置与直觉\n",
    "# ---------------------------\n",
    "# 构造一个 batch，policy 对正样本更“自信”，负样本更“不自信”\n",
    "# logp 通常为负数；数值越接近 0 表示概率越大\n",
    "policy_logp_pos = torch.tensor([-1.0, -0.2, -2.0, -0.5])  # 好回答 y+\n",
    "policy_logp_neg = torch.tensor([-2.0, -1.5, -3.0, -1.0])  # 坏回答 y-\n",
    "\n",
    "# 参考模型（ref）与 policy 差不多或者更弱\n",
    "ref_logp_pos = torch.tensor([-1.2, -0.5, -2.5, -0.7])\n",
    "ref_logp_neg = torch.tensor([-1.8, -1.3, -2.7, -0.9])\n",
    "\n",
    "# ---------------------------\n",
    "# Case 1: reference_free = True\n",
    "# 期望：loss1 较小（policy 确实更偏向 y+）\n",
    "# ---------------------------\n",
    "loss1 = dpo_loss(policy_logp_pos, policy_logp_neg, beta=0.1, reference_free=True)\n",
    "print(\"Case 1 (reference_free=True) loss:\", float(loss1))\n",
    "\n",
    "# ---------------------------\n",
    "# Case 2: 有 reference，且 policy 与 reference 相近\n",
    "# 由于两者优势差不多，logits 变小 → 损失略变大\n",
    "# ---------------------------\n",
    "loss2 = dpo_loss(policy_logp_pos, policy_logp_neg,\n",
    "                 ref_logp_pos, ref_logp_neg, beta=0.1, reference_free=False)\n",
    "print(\"Case 2 (with reference, similar preference) loss:\", float(loss2))\n",
    "\n",
    "# ---------------------------\n",
    "# Case 3: 构造“困难模式”\n",
    "# reference 更偏向负样本（或更不偏向正样本），\n",
    "# 这会放大 policy 相对 reference 的优势 → 损失更小\n",
    "# ---------------------------\n",
    "ref_bad_pos = torch.tensor([-3.0, -2.5, -3.5, -2.0])  # ref 认为 y+ 很差\n",
    "ref_bad_neg = torch.tensor([-1.0, -0.9, -1.1, -0.8])  # ref 认为 y- 还不错\n",
    "loss3 = dpo_loss(policy_logp_pos, policy_logp_neg,\n",
    "                 ref_bad_pos, ref_bad_neg, beta=0.1, reference_free=False)\n",
    "print(\"Case 3 (with reference, opposite preference) loss:\", float(loss3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d352fa1d",
   "metadata": {},
   "source": [
    "### 4. 怎么得到 `log π(y|x)`？\n",
    "\n",
    "- 模型输出是 `[B, T, V]` 的 logits。\n",
    "- 对齐 `logits[:, :-1]` 和 `labels[:, 1:]`（next-token 预测）。\n",
    "- 用 `labels == -100` mask 掉 prompt 部分，只累计回答部分的 log-prob。\n",
    "- `sum` 得到整条回答的 log-prob。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cyclegan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
