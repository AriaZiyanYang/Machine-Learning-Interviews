{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4173d05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class ActivationFunctions:\n",
    "    \"\"\"æ¿€æ´»å‡½æ•°ç±»\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid(self, x):\n",
    "        \"\"\"Sigmoidæ¿€æ´»å‡½æ•°\"\"\"\n",
    "        res = 1/(1 + np.exp(-x))\n",
    "        return res\n",
    "    \n",
    "    @staticmethod\n",
    "    def softmax(self, values):\n",
    "        \"\"\"Softmaxæ¿€æ´»å‡½æ•°\"\"\"\n",
    "        exp_values = np.exp(values)\n",
    "        sum_values = np.sum(exp_values)\n",
    "        return exp_values / sum_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e085dc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossFunctions:\n",
    "    \"\"\"æŸå¤±å‡½æ•°ç±»\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def compute_loss_multiclass(Y, Y_hat):\n",
    "        \"\"\"\n",
    "        å¤šåˆ†ç±»äº¤å‰ç†µæŸå¤± (Cross-Entropy for Multi-class Classification)\n",
    "        \n",
    "        å‚æ•°:\n",
    "        - Y: shape = (m,)  æ¯ä¸ªæ ·æœ¬çš„çœŸå®ç±»åˆ«ï¼ˆæ•´æ•°æ ‡ç­¾ï¼Œå¦‚ 0,1,2,...ï¼‰\n",
    "        - Y_hat: shape = (m, num_classes) æ¯ä¸ªæ ·æœ¬å¯¹å„ç±»åˆ«çš„é¢„æµ‹æ¦‚ç‡ï¼ˆç»è¿‡softmaxï¼‰\n",
    "        \n",
    "        è¿”å›:\n",
    "        - loss: float å¹³å‡äº¤å‰ç†µæŸå¤±\n",
    "        \"\"\"\n",
    "        m = Y.shape[0]                       # æ ·æœ¬æ•°\n",
    "        log_likelihood = -np.log(Y_hat[range(m), Y])  # å–å‡ºçœŸå®ç±»åˆ«çš„é¢„æµ‹æ¦‚ç‡å¹¶å–å¯¹æ•°\n",
    "        loss = np.sum(log_likelihood) / m             # å¹³å‡æŸå¤±\n",
    "        return loss\n",
    "    \n",
    "    @staticmethod\n",
    "    def compute_loss_binary(Y, Y_hat):\n",
    "        \"\"\"\n",
    "        äºŒåˆ†ç±»äº¤å‰ç†µæŸå¤± (Binary Cross-Entropy Loss)\n",
    "        \n",
    "        å‚æ•°:\n",
    "        - Y: shape = (m,)  æ¯ä¸ªæ ·æœ¬çš„çœŸå®æ ‡ç­¾ï¼ˆ0æˆ–1ï¼‰\n",
    "        - Y_hat: shape = (m,) æ¯ä¸ªæ ·æœ¬é¢„æµ‹ä¸ºæ­£ç±»(1)çš„æ¦‚ç‡ï¼ˆé€šå¸¸sigmoidè¾“å‡ºï¼‰\n",
    "        \n",
    "        è¿”å›:\n",
    "        - loss: float å¹³å‡äº¤å‰ç†µæŸå¤±\n",
    "        \"\"\"\n",
    "        # é¿å…log(0)ï¼ŒåŠ ä¸Šä¸€ä¸ªæå°å€¼epsilon\n",
    "        eps = 1e-15\n",
    "        Y_hat = np.clip(Y_hat, eps, 1 - eps)\n",
    "        loss = -np.mean(Y * np.log(Y_hat) + (1 - Y) * np.log(1 - Y_hat))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d13aa6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ§ª æµ‹è¯•å¤šåˆ†ç±»æŸå¤±å‡½æ•°\n",
    "Y = np.array([0, 1, 2])   # ä¸‰ä¸ªæ ·æœ¬ï¼ŒçœŸå®æ ‡ç­¾åˆ†åˆ«æ˜¯ç±»åˆ«0ã€1ã€2\n",
    "Y_hat = np.array([\n",
    "    [0.7, 0.2, 0.1],      # å¯¹ç¬¬ä¸€ä¸ªæ ·æœ¬çš„é¢„æµ‹\n",
    "    [0.1, 0.8, 0.1],      # ç¬¬äºŒä¸ªæ ·æœ¬\n",
    "    [0.2, 0.3, 0.5]       # ç¬¬ä¸‰ä¸ªæ ·æœ¬\n",
    "])\n",
    "\n",
    "loss_multi = LossFunctions.compute_loss_multiclass(Y, Y_hat)\n",
    "print(\"ğŸ”¹Multi-class Cross Entropy Loss:\", loss_multi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc83bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ§ª æµ‹è¯•äºŒåˆ†ç±»æŸå¤±å‡½æ•°\n",
    "Y_binary = np.array([1, 0, 1, 0])        # çœŸå®æ ‡ç­¾\n",
    "Y_hat_binary = np.array([0.9, 0.2, 0.8, 0.1])  # æ¨¡å‹é¢„æµ‹æ¦‚ç‡(sigmoidè¾“å‡º)\n",
    "\n",
    "loss_binary = LossFunctions.compute_loss_binary(Y_binary, Y_hat_binary)\n",
    "print(\"ğŸ”¹Binary Cross Entropy Loss:\", loss_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46629610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ§ª æµ‹è¯•æ¿€æ´»å‡½æ•°\n",
    "print(\"ğŸ”¹æµ‹è¯•æ¿€æ´»å‡½æ•°:\")\n",
    "x = np.array([1, 2, 3])\n",
    "sigmoid_output = ActivationFunctions.sigmoid(None, x)\n",
    "softmax_output = ActivationFunctions.softmax(None, x)\n",
    "\n",
    "print(f\"Sigmoid({x}) = {sigmoid_output}\")\n",
    "print(f\"Softmax({x}) = {softmax_output}\")\n",
    "print(f\"Softmaxæ¦‚ç‡å’Œ: {np.sum(softmax_output)}\")  # åº”è¯¥ç­‰äº1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f483f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“š ä½¿ç”¨ç¤ºä¾‹æ€»ç»“\n",
    "print(\"\\nğŸ“š ç±»ç»“æ„æ€»ç»“:\")\n",
    "print(\"1. ActivationFunctions ç±»:\")\n",
    "print(\"   - sigmoid(): äºŒåˆ†ç±»æ¿€æ´»å‡½æ•°\")\n",
    "print(\"   - softmax(): å¤šåˆ†ç±»æ¿€æ´»å‡½æ•°\")\n",
    "print(\"\\n2. LossFunctions ç±»:\")\n",
    "print(\"   - compute_loss_binary(): äºŒåˆ†ç±»äº¤å‰ç†µæŸå¤±\")\n",
    "print(\"   - compute_loss_multiclass(): å¤šåˆ†ç±»äº¤å‰ç†µæŸå¤±\")\n",
    "print(\"\\nâœ… ç°åœ¨æ‰€æœ‰å‡½æ•°éƒ½ç»„ç»‡åœ¨ç±»ä¸­ï¼Œä½¿ç”¨æ›´åŠ è§„èŒƒï¼\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
