{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression \n",
    "\n",
    "Logistic regression is a statistical method used for binary classification, which means it is used to predict the probability of an event occurring or not. It is a type of generalized linear model that is used when the dependent variable is binary or categorical.\n",
    "\n",
    "In logistic regression, the dependent variable is binary (i.e., it can take on one of two values, usually 0 or 1), and the independent variables can be either continuous or categorical. The goal of logistic regression is to find the relationship between the independent variables and the dependent variable by estimating the probability of the dependent variable being 1 given the values of the independent variables.\n",
    "\n",
    "The logistic regression model uses a logistic function (also known as the sigmoid function) to map the input values of the independent variables to a value between 0 and 1, which represents the probability of the dependent variable being 1. The logistic function is defined as:\n",
    "\n",
    "css\n",
    "Copy code\n",
    "p = 1 / (1 + e^(-z))\n",
    "where p is the predicted probability of the dependent variable being 1, e is the base of the natural logarithm, and z is the linear combination of the independent variables.\n",
    "\n",
    "The logistic regression model estimates the values of the coefficients of the independent variables that maximize the likelihood of observing the data given the model. This is typically done using maximum likelihood estimation or gradient descent optimization.\n",
    "\n",
    "Once the model is trained, it can be used to make predictions on new data by inputting the values of the independent variables into the logistic function and obtaining the predicted probability of the dependent variable being 1. The model can then classify the new observation as 1 or 0 based on a threshold probability value that is chosen by the user.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code \n",
    " Here's an example implementation using gradient descent optimization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  # 导入numpy库，用于数值计算 / Import numpy library for numerical computations\n",
    "\n",
    "class LogisticRegression:  # 二分类逻辑回归类 / Binary classification logistic regression class\n",
    "    \n",
    "    def __init__(self, learning_rate=0.01, n_iters=1000):  # 构造函数 / Constructor\n",
    "        self.learning_rate = learning_rate  # 学习率 / Learning rate\n",
    "        self.n_iters = n_iters  # 最大迭代次数 / Maximum number of iterations\n",
    "        self.weights = None  # 权重向量 / Weight vector\n",
    "        self.bias = None  # 偏置项 / Bias term\n",
    "        \n",
    "    def fit(self, X, y):  # 训练方法 / Training method\n",
    "        # 初始化权重和偏置为零 / Initialize weights and bias to zeros\n",
    "        n_samples, n_features = X.shape  # 获取样本数和特征数 / Get number of samples and features\n",
    "        self.weights = np.zeros(n_features)  # 初始化权重为零向量 / Initialize weights as zero vector\n",
    "        self.bias = 0  # 初始化偏置为零 / Initialize bias to zero\n",
    "        \n",
    "        # 梯度下降优化 / Gradient descent optimization\n",
    "        for i in range(self.n_iters):  # 迭代训练 / Iterative training\n",
    "            # 计算预测概率和损失 / Calculate predicted probabilities and cost\n",
    "            z = np.dot(X, self.weights) + self.bias  # 线性组合 / Linear combination\n",
    "            y_pred = self._sigmoid(z)  # 应用sigmoid函数 / Apply sigmoid function\n",
    "            cost = (-1 / n_samples) * np.sum(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred))  # 交叉熵损失 / Cross-entropy loss\n",
    "            \n",
    "            # 计算梯度 / Calculate gradients\n",
    "            dw = (1 / n_samples) * np.dot(X.T, (y_pred - y))  # 权重梯度 / Weight gradient\n",
    "            db = (1 / n_samples) * np.sum(y_pred - y)  # 偏置梯度 / Bias gradient\n",
    "            \n",
    "            # 更新权重和偏置 / Update weights and bias\n",
    "            self.weights -= self.learning_rate * dw  # 更新权重 / Update weights\n",
    "            self.bias -= self.learning_rate * db  # 更新偏置 / Update bias\n",
    "            \n",
    "    def predict(self, X):  # 预测方法 / Prediction method\n",
    "        # 计算预测概率 / Calculate predicted probabilities\n",
    "        z = np.dot(X, self.weights) + self.bias  # 线性组合 / Linear combination\n",
    "        y_pred = self._sigmoid(z)  # 应用sigmoid函数 / Apply sigmoid function\n",
    "        # 将概率转换为二进制预测 / Convert probabilities to binary predictions\n",
    "        return np.round(y_pred).astype(int)  # 四舍五入为整数 / Round to integers\n",
    "    \n",
    "    def _sigmoid(self, z):  # sigmoid激活函数 / sigmoid activation function\n",
    "        return 1 / (1 + np.exp(-z))  # sigmoid公式 / sigmoid formula\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1]\n"
     ]
    }
   ],
   "source": [
    "# 创建样本数据集 / Create sample dataset\n",
    "X = np.array([[1, 2], [2, 3], [3, 4], [4, 5], [5, 6]])  # 特征矩阵，5个样本，每个样本2个特征 / Feature matrix: 5 samples, 2 features each\n",
    "y = np.array([0, 0, 1, 1, 1])  # 标签向量，二分类标签 / Label vector: binary classification labels\n",
    "\n",
    "# 初始化逻辑回归模型 / Initialize logistic regression model\n",
    "lr = LogisticRegression()  # 使用默认参数创建模型 / Create model with default parameters\n",
    "\n",
    "# 在样本数据集上训练模型 / Train model on sample dataset\n",
    "lr.fit(X, y)  # 训练模型 / Train the model\n",
    "\n",
    "# 对新数据进行预测 / Make predictions on new data\n",
    "X_new = np.array([[6, 7], [7, 8]])  # 新的测试数据 / New test data\n",
    "y_pred = lr.predict(X_new)  # 进行预测 / Make predictions\n",
    "\n",
    "print(y_pred)  # 打印预测结果 / Print prediction results\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improvements \n",
    "here are some possible improvements you could make to the code:\n",
    "\n",
    "1. Add regularization: Regularization can help prevent overfitting and improve the generalization performance of the model. You could add L1 or L2 regularization to the cost function and adjust the regularization strength with a hyperparameter. Here's an example of how to add L2 regularization to the code:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Use a more sophisticated optimization algorithm: Gradient descent is a simple and effective optimization algorithm, but it may not be the most efficient or accurate for large or complex datasets. You could try using a more sophisticated algorithm, such as stochastic gradient descent (SGD), mini-batch SGD, or Adam, which can converge faster and find better optima. Here's an example of how to use mini-batch SGD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  # 导入numpy库，用于数值计算 / Import numpy library for numerical computations\n",
    "\n",
    "class LogisticRegression:  # 改进版逻辑回归类，支持正则化和mini-batch SGD / Enhanced logistic regression class with regularization and mini-batch SGD\n",
    "    \n",
    "    def __init__(self, learning_rate=0.01, n_iters=1000, regularization='l2', reg_strength=0.1, batch_size=32):  # 构造函数 / Constructor\n",
    "        self.learning_rate = learning_rate  # 学习率 / Learning rate\n",
    "        self.n_iters = n_iters  # 最大迭代次数 / Maximum number of iterations\n",
    "        self.regularization = regularization  # 正则化类型：'l1'或'l2' / Regularization type: 'l1' or 'l2'\n",
    "        self.reg_strength = reg_strength  # 正则化强度 / Regularization strength\n",
    "        self.batch_size = batch_size  # mini-batch大小 / Mini-batch size\n",
    "        self.weights = None  # 权重向量 / Weight vector\n",
    "        self.bias = None  # 偏置项 / Bias term\n",
    "        \n",
    "    def fit(self, X, y):  # 训练方法 / Training method\n",
    "        n_samples, n_features = X.shape  # 获取样本数和特征数 / Get number of samples and features\n",
    "        self.weights = np.zeros(n_features)  # 初始化权重为零向量 / Initialize weights as zero vector\n",
    "        self.bias = 0  # 初始化偏置为零 / Initialize bias to zero\n",
    "        n_batches = n_samples // self.batch_size  # 计算batch数量 / Calculate number of batches\n",
    "        for i in range(self.n_iters):  # 迭代训练 / Iterative training\n",
    "            batch_indices = np.random.choice(n_samples, self.batch_size)  # 随机选择batch索引 / Randomly select batch indices\n",
    "            X_batch = X[batch_indices]  # 获取batch特征 / Get batch features\n",
    "            y_batch = y[batch_indices]  # 获取batch标签 / Get batch labels\n",
    "            z = np.dot(X_batch, self.weights) + self.bias  # 线性组合 / Linear combination\n",
    "            y_pred = self._sigmoid(z)  # 应用sigmoid函数 / Apply sigmoid function\n",
    "            cost = (-1 / self.batch_size) * np.sum(y_batch * np.log(y_pred) + (1 - y_batch) * np.log(1 - y_pred))  # 交叉熵损失 / Cross-entropy loss\n",
    "            if self.regularization == 'l2':  # L2正则化 / L2 regularization\n",
    "                reg_cost = (self.reg_strength / (2 * n_samples)) * np.sum(self.weights ** 2)  # L2正则化项 / L2 regularization term\n",
    "                cost += reg_cost  # 添加到损失 / Add to cost\n",
    "            elif self.regularization == 'l1':  # L1正则化 / L1 regularization\n",
    "                reg_cost = (self.reg_strength / (2 * n_samples)) * np.sum(np.abs(self.weights))  # L1正则化项 / L1 regularization term\n",
    "                cost += reg_cost  # 添加到损失 / Add to cost\n",
    "            dw = (1 / self.batch_size) * np.dot(X_batch.T, (y_pred - y_batch))  # 权重梯度 / Weight gradient\n",
    "            db = (1 / self.batch_size) * np.sum(y_pred - y_batch)  # 偏置梯度 / Bias gradient\n",
    "            if self.regularization == 'l2':  # L2正则化梯度 / L2 regularization gradient\n",
    "                dw += (self.reg_strength / n_samples) * self.weights  # 添加L2正则化梯度 / Add L2 regularization gradient\n",
    "            elif self.regularization == 'l1':  # L1正则化梯度 / L1 regularization gradient\n",
    "                dw += (self.reg_strength / n_samples) * np.sign(self.weights)  # 添加L1正则化梯度 / Add L1 regularization gradient\n",
    "            self.weights -= self.learning_rate * dw  # 更新权重 / Update weights\n",
    "            self.bias -= self.learning_rate * db  # 更新偏置 / Update bias\n",
    "            \n",
    "    def predict(self, X):  # 预测方法 / Prediction method\n",
    "        z = np.dot(X, self.weights) + self.bias  # 线性组合 / Linear combination\n",
    "        y_pred = self._sigmoid(z)  # 应用sigmoid函数 / Apply sigmoid function\n",
    "        return np.round(y_pred).astype(int)  # 四舍五入为整数 / Round to integers\n",
    "    \n",
    "    def _sigmoid(self, z):  # sigmoid激活函数 / sigmoid activation function\n",
    "        return 1 / (1 + np.exp(-z))  # sigmoid公式 / sigmoid formula\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This implementation includes the following improvements:\n",
    "\n",
    "1. Regularization: You can choose between L1 or L2 regularization by setting the regularization parameter to either 'l1' or 'l2', and adjust the regularization strength with the reg_strength parameter.\n",
    "\n",
    "2. Mini-batch stochastic gradient descent: The model uses mini-batch SGD (instead of simple gradient descent) to update the weights and bias, which can converge faster and find better optima.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1]\n"
     ]
    }
   ],
   "source": [
    "# 创建样本数据集 / Create sample dataset\n",
    "X = np.array([[1, 2], [2, 3], [3, 4], [4, 5], [5, 6]])  # 特征矩阵，5个样本，每个样本2个特征 / Feature matrix: 5 samples, 2 features each\n",
    "y = np.array([0, 0, 1, 1, 1])  # 标签向量，二分类标签 / Label vector: binary classification labels\n",
    "\n",
    "# 初始化改进版逻辑回归模型 / Initialize enhanced logistic regression model\n",
    "lr = LogisticRegression(learning_rate=0.01, n_iters=1000, regularization='l2', reg_strength=0.1, batch_size=2)  # 使用L2正则化和mini-batch SGD / Use L2 regularization and mini-batch SGD\n",
    "\n",
    "# 在样本数据集上训练模型 / Train model on sample dataset\n",
    "lr.fit(X, y)  # 训练模型 / Train the model\n",
    "\n",
    "# 对新数据进行预测 / Make predictions on new data\n",
    "X_new = np.array([[6, 7], [7, 8]])  # 新的测试数据 / New test data\n",
    "y_pred = lr.predict(X_new)  # 进行预测 / Make predictions\n",
    "\n",
    "print(y_pred)  # 打印预测结果 / Print prediction results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is difficult to visualize logistic regression since it is a high-dimensional problem. However, we can visualize the decision boundary of a logistic regression model for a two-dimensional dataset.\n",
    "\n",
    "Here's an example of how to visualize the decision boundary of the LogisticRegression class on a 2D dataset using the matplotlib library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt  # 导入matplotlib库，用于绘图 / Import matplotlib library for plotting\n",
    "\n",
    "# 创建2D数据集 / Create 2D dataset\n",
    "X = np.array([[1, 2], [2, 3], [3, 4], [4, 5], [5, 6]])  # 特征矩阵，5个样本，每个样本2个特征 / Feature matrix: 5 samples, 2 features each\n",
    "y = np.array([0, 0, 1, 1, 1])  # 标签向量，二分类标签 / Label vector: binary classification labels\n",
    "\n",
    "# 初始化逻辑回归模型 / Initialize logistic regression model\n",
    "lr = LogisticRegression(learning_rate=0.01, n_iters=1000, regularization='l2', reg_strength=0.1, batch_size=2)  # 使用L2正则化和mini-batch SGD / Use L2 regularization and mini-batch SGD\n",
    "\n",
    "# 在数据集上训练模型 / Train model on dataset\n",
    "lr.fit(X, y)  # 训练模型 / Train the model\n",
    "\n",
    "# 绘制决策边界 / Plot decision boundary\n",
    "x1 = np.linspace(0, 6, 100)  # 创建x1坐标网格 / Create x1 coordinate grid\n",
    "x2 = np.linspace(0, 8, 100)  # 创建x2坐标网格 / Create x2 coordinate grid\n",
    "xx, yy = np.meshgrid(x1, x2)  # 创建网格点 / Create grid points\n",
    "Z = lr.predict(np.c_[xx.ravel(), yy.ravel()])  # 对网格点进行预测 / Predict on grid points\n",
    "Z = Z.reshape(xx.shape)  # 重塑预测结果为网格形状 / Reshape predictions to grid shape\n",
    "plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral, alpha=0.8)  # 绘制决策边界填充图 / Plot decision boundary filled contour\n",
    "\n",
    "# 绘制数据点 / Plot data points\n",
    "plt.scatter(X[:,0], X[:,1], c=y, cmap=plt.cm.Spectral)  # 绘制原始数据点，用颜色表示类别 / Plot original data points with color-coded classes\n",
    "\n",
    "plt.show()  # 显示图形 / Display the plot"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cyclegan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
