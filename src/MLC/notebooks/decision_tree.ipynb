{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Implementation / å†³ç­–æ ‘å®ç°\n",
    "\n",
    "## Algorithm Overview / ç®—æ³•æ¦‚è¿°\n",
    "\n",
    "A decision tree is a type of machine learning algorithm used for classification and regression tasks. It consists of a tree-like structure where each internal node represents a feature or attribute, each branch represents a decision based on that feature, and each leaf node represents a predicted output.\n",
    "\n",
    "å†³ç­–æ ‘æ˜¯ä¸€ç§ç”¨äºåˆ†ç±»å’Œå›å½’ä»»åŠ¡çš„æœºå™¨å­¦ä¹ ç®—æ³•ã€‚å®ƒç”±æ ‘çŠ¶ç»“æ„ç»„æˆï¼Œå…¶ä¸­æ¯ä¸ªå†…éƒ¨èŠ‚ç‚¹ä»£è¡¨ä¸€ä¸ªç‰¹å¾æˆ–å±æ€§ï¼Œæ¯ä¸ªåˆ†æ”¯ä»£è¡¨åŸºäºè¯¥ç‰¹å¾çš„å†³ç­–ï¼Œæ¯ä¸ªå¶èŠ‚ç‚¹ä»£è¡¨é¢„æµ‹è¾“å‡ºã€‚\n",
    "\n",
    "## Training Process / è®­ç»ƒè¿‡ç¨‹\n",
    "\n",
    "To **train** a decision tree, the algorithm uses a dataset with labeled examples to create the tree structure. It starts with the root node, which includes all the examples, and selects the feature that provides the most information gain to split the data into two subsets. It then repeats this process for each subset until it reaches a stopping criterion, such as a maximum tree depth or minimum number of examples in a leaf node.\n",
    "\n",
    "è¦**è®­ç»ƒ**å†³ç­–æ ‘ï¼Œç®—æ³•ä½¿ç”¨å¸¦æœ‰æ ‡ç­¾ç¤ºä¾‹çš„æ•°æ®é›†æ¥åˆ›å»ºæ ‘ç»“æ„ã€‚å®ƒä»åŒ…å«æ‰€æœ‰ç¤ºä¾‹çš„æ ¹èŠ‚ç‚¹å¼€å§‹ï¼Œé€‰æ‹©æä¾›æœ€å¤§ä¿¡æ¯å¢ç›Šçš„ç‰¹å¾å°†æ•°æ®åˆ†å‰²æˆä¸¤ä¸ªå­é›†ã€‚ç„¶åå¯¹æ¯ä¸ªå­é›†é‡å¤æ­¤è¿‡ç¨‹ï¼Œç›´åˆ°è¾¾åˆ°åœæ­¢æ ‡å‡†ï¼Œå¦‚æœ€å¤§æ ‘æ·±åº¦æˆ–å¶èŠ‚ç‚¹ä¸­çš„æœ€å°ç¤ºä¾‹æ•°ã€‚\n",
    "\n",
    "## Prediction Process / é¢„æµ‹è¿‡ç¨‹\n",
    "\n",
    "Once the decision tree is trained, it can be used to **predict** the output for new, unseen examples. To make a prediction, the algorithm starts at the root node and follows the branches based on the values of the input features until it reaches a leaf node. The predicted output for that example is the value associated with the leaf node.\n",
    "\n",
    "ä¸€æ—¦å†³ç­–æ ‘è®­ç»ƒå®Œæˆï¼Œå°±å¯ä»¥ç”¨æ¥**é¢„æµ‹**æ–°çš„ã€æœªè§è¿‡çš„ç¤ºä¾‹çš„è¾“å‡ºã€‚è¦è¿›è¡Œé¢„æµ‹ï¼Œç®—æ³•ä»æ ¹èŠ‚ç‚¹å¼€å§‹ï¼Œæ ¹æ®è¾“å…¥ç‰¹å¾çš„å€¼æ²¿ç€åˆ†æ”¯ï¼Œç›´åˆ°åˆ°è¾¾å¶èŠ‚ç‚¹ã€‚è¯¥ç¤ºä¾‹çš„é¢„æµ‹è¾“å‡ºæ˜¯ä¸å¶èŠ‚ç‚¹å…³è”çš„å€¼ã€‚\n",
    "\n",
    "## Advantages and Limitations / ä¼˜ç¼ºç‚¹\n",
    "\n",
    "**Advantages / ä¼˜ç‚¹:**\n",
    "- Easy to interpret and visualize / æ˜“äºè§£é‡Šå’Œå¯è§†åŒ–\n",
    "- Handle both numerical and categorical data / å¤„ç†æ•°å€¼å’Œåˆ†ç±»æ•°æ®\n",
    "- Handle missing values / å¤„ç†ç¼ºå¤±å€¼\n",
    "\n",
    "**Limitations / å±€é™æ€§:**\n",
    "- Can suffer from overfitting / å¯èƒ½è¿‡æ‹Ÿåˆ\n",
    "- May not perform well with highly imbalanced datasets / åœ¨é«˜åº¦ä¸å¹³è¡¡çš„æ•°æ®é›†ä¸Šå¯èƒ½è¡¨ç°ä¸ä½³\n",
    "- Not suitable for highly nonlinear relationships / ä¸é€‚åˆé«˜åº¦éçº¿æ€§å…³ç³»\n",
    "\n",
    "## Interview Clarification Questions / é¢è¯•æ¾„æ¸…é—®é¢˜\n",
    "\n",
    "**ğŸ” Key Questions to Ask:**\n",
    "1. **Data Type**: Numerical only or categorical too? / æ•°æ®ç±»å‹ï¼šä»…æ•°å€¼è¿˜æ˜¯åŒ…æ‹¬åˆ†ç±»ï¼Ÿ\n",
    "2. **Splitting Criteria**: Gini, Entropy, or MSE? / åˆ†å‰²æ ‡å‡†ï¼šåŸºå°¼ã€ç†µè¿˜æ˜¯å‡æ–¹è¯¯å·®ï¼Ÿ\n",
    "3. **Stopping Conditions**: Max depth, min samples, min impurity? / åœæ­¢æ¡ä»¶ï¼šæœ€å¤§æ·±åº¦ã€æœ€å°æ ·æœ¬ã€æœ€å°ä¸çº¯åº¦ï¼Ÿ\n",
    "4. **Missing Values**: How to handle? / ç¼ºå¤±å€¼ï¼šå¦‚ä½•å¤„ç†ï¼Ÿ\n",
    "5. **Multi-class**: Binary or multi-class classification? / å¤šç±»ï¼šäºŒåˆ†ç±»è¿˜æ˜¯å¤šåˆ†ç±»ï¼Ÿ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  # Import numpy for numerical computations / å¯¼å…¥numpyåº“ç”¨äºæ•°å€¼è®¡ç®—\n",
    "\n",
    "class DecisionTree:\n",
    "    \"\"\"Decision Tree Classifier Implementation / å†³ç­–æ ‘åˆ†ç±»å™¨å®ç°\n",
    "    \n",
    "    ğŸ” Interview Clarification Questions / é¢è¯•æ¾„æ¸…é—®é¢˜:\n",
    "    1. Should we support both classification and regression? / æ˜¯å¦æ”¯æŒåˆ†ç±»å’Œå›å½’ï¼Ÿ\n",
    "    2. What splitting criteria to use? (Gini, Entropy, MSE) / ä½¿ç”¨ä»€ä¹ˆåˆ†å‰²æ ‡å‡†ï¼Ÿ\n",
    "    3. How to handle categorical features? / å¦‚ä½•å¤„ç†åˆ†ç±»ç‰¹å¾ï¼Ÿ\n",
    "    4. What stopping conditions? / ä»€ä¹ˆåœæ­¢æ¡ä»¶ï¼Ÿ\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, max_depth=None):\n",
    "        \"\"\"Initialize Decision Tree / åˆå§‹åŒ–å†³ç­–æ ‘\n",
    "        \n",
    "        Args / å‚æ•°:\n",
    "            max_depth: Maximum tree depth, None means unlimited / æ ‘çš„æœ€å¤§æ·±åº¦ï¼ŒNoneè¡¨ç¤ºæ— é™åˆ¶\n",
    "            \n",
    "        ğŸ” Interview Questions / é¢è¯•é—®é¢˜:\n",
    "        - Why limit depth? (Prevent overfitting) / ä¸ºä»€ä¹ˆé™åˆ¶æ·±åº¦ï¼Ÿï¼ˆé˜²æ­¢è¿‡æ‹Ÿåˆï¼‰\n",
    "        - What other parameters might be useful? / å…¶ä»–æœ‰ç”¨å‚æ•°ï¼Ÿ\n",
    "        \"\"\"\n",
    "        self.max_depth = max_depth  # Store max depth parameter / å­˜å‚¨æœ€å¤§æ·±åº¦å‚æ•°\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Train the decision tree / è®­ç»ƒå†³ç­–æ ‘\n",
    "        \n",
    "        Args / å‚æ•°:\n",
    "            X: Feature matrix (n_samples, n_features) / ç‰¹å¾çŸ©é˜µ\n",
    "            y: Label vector (n_samples,) / æ ‡ç­¾å‘é‡\n",
    "            \n",
    "        ğŸ” Interview Questions / é¢è¯•é—®é¢˜:\n",
    "        - Should we validate input data? / æ˜¯å¦éªŒè¯è¾“å…¥æ•°æ®ï¼Ÿ\n",
    "        - How to handle different data types? / å¦‚ä½•å¤„ç†ä¸åŒæ•°æ®ç±»å‹ï¼Ÿ\n",
    "        - What if features have different scales? / å¦‚æœç‰¹å¾å°ºåº¦ä¸åŒæ€ä¹ˆåŠï¼Ÿ\n",
    "        \"\"\"\n",
    "        self.n_classes_ = len(np.unique(y))  # Count number of classes / è®¡ç®—ç±»åˆ«æ•°é‡\n",
    "        self.n_features_ = X.shape[1]        # Get number of features / è·å–ç‰¹å¾æ•°é‡\n",
    "        self.tree_ = self._grow_tree(X, y)   # Recursively build decision tree / é€’å½’æ„å»ºå†³ç­–æ ‘\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict classes for new samples / é¢„æµ‹æ–°æ ·æœ¬çš„ç±»åˆ«\n",
    "        \n",
    "        Args / å‚æ•°:\n",
    "            X: Feature matrix (n_samples, n_features) / ç‰¹å¾çŸ©é˜µ\n",
    "            \n",
    "        Returns / è¿”å›:\n",
    "            List of predicted classes / é¢„æµ‹çš„ç±»åˆ«åˆ—è¡¨\n",
    "            \n",
    "        ğŸ” Interview Questions / é¢è¯•é—®é¢˜:\n",
    "        - Should we return probabilities too? / æ˜¯å¦ä¹Ÿè¿”å›æ¦‚ç‡ï¼Ÿ\n",
    "        - How to handle batch vs single prediction? / å¦‚ä½•å¤„ç†æ‰¹é‡vså•ä¸ªé¢„æµ‹ï¼Ÿ\n",
    "        - What if tree is not trained? / å¦‚æœæ ‘æœªè®­ç»ƒæ€ä¹ˆåŠï¼Ÿ\n",
    "        \"\"\"\n",
    "        return [self._predict(inputs) for inputs in X]  # Predict each sample / å¯¹æ¯ä¸ªæ ·æœ¬è¿›è¡Œé¢„æµ‹\n",
    "        \n",
    "    def _gini(self, y):\n",
    "        \"\"\"Calculate Gini impurity / è®¡ç®—åŸºå°¼ä¸çº¯åº¦\n",
    "        \n",
    "        Args / å‚æ•°:\n",
    "            y: Label array / æ ‡ç­¾æ•°ç»„\n",
    "            \n",
    "        Returns / è¿”å›:\n",
    "            Gini impurity value (0-1, 0 means perfectly pure) / åŸºå°¼ä¸çº¯åº¦å€¼ (0-1ä¹‹é—´ï¼Œ0è¡¨ç¤ºå®Œå…¨çº¯å‡€)\n",
    "            \n",
    "        ğŸ” Interview Questions / é¢è¯•é—®é¢˜:\n",
    "        - Why Gini over Entropy? / ä¸ºä»€ä¹ˆé€‰æ‹©åŸºå°¼è€Œä¸æ˜¯ç†µï¼Ÿ\n",
    "        - How does Gini behave with imbalanced classes? / åŸºå°¼åœ¨ä¸å¹³è¡¡ç±»åˆ«ä¸­å¦‚ä½•è¡¨ç°ï¼Ÿ\n",
    "        - What's the computational complexity? / è®¡ç®—å¤æ‚åº¦æ˜¯ä»€ä¹ˆï¼Ÿ\n",
    "        \"\"\"\n",
    "        if len(y) == 0:  # If array is empty, return 0 / å¦‚æœæ•°ç»„ä¸ºç©ºï¼Œè¿”å›0\n",
    "            return 0\n",
    "        _, counts = np.unique(y, return_counts=True)  # Count each class / ç»Ÿè®¡æ¯ä¸ªç±»åˆ«çš„æ•°é‡\n",
    "        probabilities = counts / len(y)               # Calculate class probabilities / è®¡ç®—æ¯ä¸ªç±»åˆ«çš„æ¦‚ç‡\n",
    "        return 1 - np.sum(probabilities ** 2)          # Gini formula: 1 - Î£(p_i)Â² / åŸºå°¼ä¸çº¯åº¦å…¬å¼\n",
    "        \n",
    "    def _best_split(self, X, y):\n",
    "        \"\"\"Find the best split point / æ‰¾åˆ°æœ€ä½³åˆ†å‰²ç‚¹\n",
    "        \n",
    "        Args / å‚æ•°:\n",
    "            X: Feature matrix / ç‰¹å¾çŸ©é˜µ\n",
    "            y: Label array / æ ‡ç­¾æ•°ç»„\n",
    "            \n",
    "        Returns / è¿”å›:\n",
    "            (best_feature_index, best_threshold) or (None, None) / æœ€ä½³ç‰¹å¾ç´¢å¼•å’Œé˜ˆå€¼æˆ–None\n",
    "            \n",
    "        ğŸ” Interview Questions / é¢è¯•é—®é¢˜:\n",
    "        - How to handle continuous vs discrete features? / å¦‚ä½•å¤„ç†è¿ç»­vsç¦»æ•£ç‰¹å¾ï¼Ÿ\n",
    "        - What's the time complexity? / æ—¶é—´å¤æ‚åº¦æ˜¯ä»€ä¹ˆï¼Ÿ\n",
    "        - How to handle ties in impurity? / å¦‚ä½•å¤„ç†ä¸çº¯åº¦ç›¸ç­‰çš„æƒ…å†µï¼Ÿ\n",
    "        \"\"\"\n",
    "        m = y.size  # Get number of samples / è·å–æ ·æœ¬æ•°é‡\n",
    "        if m <= 1:  # If <=1 samples, cannot split / å¦‚æœæ ·æœ¬æ•°<=1ï¼Œæ— æ³•åˆ†å‰²\n",
    "            return None, None\n",
    "        \n",
    "        # Calculate parent node Gini impurity / è®¡ç®—çˆ¶èŠ‚ç‚¹çš„åŸºå°¼ä¸çº¯åº¦\n",
    "        parent_gini = self._gini(y)\n",
    "        best_gini = parent_gini  # Initialize best Gini / åˆå§‹åŒ–æœ€ä½³åŸºå°¼ä¸çº¯åº¦\n",
    "        best_idx, best_thr = None, None  # Initialize best feature and threshold / åˆå§‹åŒ–æœ€ä½³ç‰¹å¾ç´¢å¼•å’Œé˜ˆå€¼\n",
    "        \n",
    "        # Iterate through all features / éå†æ‰€æœ‰ç‰¹å¾\n",
    "        for idx in range(self.n_features_):\n",
    "            # Sort by feature values while keeping corresponding labels / æŒ‰ç‰¹å¾å€¼æ’åºï¼ŒåŒæ—¶ä¿æŒå¯¹åº”çš„æ ‡ç­¾\n",
    "            thresholds, classes = zip(*sorted(zip(X[:, idx], y)))\n",
    "            num_left = [0] * self.n_classes_  # Left subtree class counts / å·¦å­æ ‘çš„ç±»åˆ«è®¡æ•°\n",
    "            num_right = [np.sum(y == c) for c in range(self.n_classes_)]  # Right subtree class counts / å³å­æ ‘çš„ç±»åˆ«è®¡æ•°\n",
    "            \n",
    "            # Try all possible split points / å°è¯•æ‰€æœ‰å¯èƒ½çš„åˆ†å‰²ç‚¹\n",
    "            for i in range(1, m):\n",
    "                c = int(classes[i - 1])  # Get current sample class, convert to int / è·å–å½“å‰æ ·æœ¬çš„ç±»åˆ«ï¼Œè½¬æ¢ä¸ºæ•´æ•°\n",
    "                num_left[c] += 1         # Move sample to left subtree / å°†æ ·æœ¬ç§»åˆ°å·¦å­æ ‘\n",
    "                num_right[c] -= 1       # Remove sample from right subtree / ä»å³å­æ ‘ç§»é™¤æ ·æœ¬\n",
    "                \n",
    "                # Skip if thresholds are the same (avoid duplicate calculation) / å¦‚æœé˜ˆå€¼ç›¸åŒï¼Œè·³è¿‡ï¼ˆé¿å…é‡å¤è®¡ç®—ï¼‰\n",
    "                if thresholds[i] == thresholds[i - 1]:\n",
    "                    continue\n",
    "                    \n",
    "                # Calculate weighted Gini impurity / è®¡ç®—åŠ æƒåŸºå°¼ä¸çº¯åº¦\n",
    "                gini_left = self._gini(y[:i])    # Left subtree Gini / å·¦å­æ ‘çš„åŸºå°¼ä¸çº¯åº¦\n",
    "                gini_right = self._gini(y[i:])   # Right subtree Gini / å³å­æ ‘çš„åŸºå°¼ä¸çº¯åº¦\n",
    "                weighted_gini = (i * gini_left + (m - i) * gini_right) / m  # Weighted average / åŠ æƒå¹³å‡\n",
    "                \n",
    "                # If found better split, update best split / å¦‚æœæ‰¾åˆ°æ›´å¥½çš„åˆ†å‰²ç‚¹ï¼Œæ›´æ–°æœ€ä½³åˆ†å‰²\n",
    "                if weighted_gini < best_gini:\n",
    "                    best_gini = weighted_gini\n",
    "                    best_idx = idx\n",
    "                    best_thr = (thresholds[i] + thresholds[i - 1]) / 2  # Use midpoint as threshold / å–ä¸­ç‚¹ä½œä¸ºé˜ˆå€¼\n",
    "        \n",
    "        return best_idx, best_thr  # Return best split feature and threshold / è¿”å›æœ€ä½³åˆ†å‰²ç‰¹å¾å’Œé˜ˆå€¼\n",
    "        \n",
    "    def _grow_tree(self, X, y, depth=0):\n",
    "        \"\"\"é€’å½’æ„å»ºå†³ç­–æ ‘\n",
    "        Args:\n",
    "            X: ç‰¹å¾çŸ©é˜µ\n",
    "            y: æ ‡ç­¾æ•°ç»„\n",
    "            depth: å½“å‰æ·±åº¦\n",
    "        Returns:\n",
    "            æ ‘çš„æ ¹èŠ‚ç‚¹\n",
    "        \"\"\"\n",
    "        # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„æ ·æœ¬æ•°é‡\n",
    "        num_samples_per_class = [np.sum(y == i) for i in range(self.n_classes_)]\n",
    "        predicted_class = np.argmax(num_samples_per_class)  # é€‰æ‹©æ ·æœ¬æœ€å¤šçš„ç±»åˆ«ä½œä¸ºé¢„æµ‹\n",
    "        node = Node(predicted_class=predicted_class)        # åˆ›å»ºæ–°èŠ‚ç‚¹\n",
    "        \n",
    "        # æ£€æŸ¥æ˜¯å¦åº”è¯¥ç»§ç»­åˆ†å‰²\n",
    "        should_split = (\n",
    "            (self.max_depth is None or depth < self.max_depth) and  # æ·±åº¦é™åˆ¶\n",
    "            len(y) > 1 and                                          # æ ·æœ¬æ•°é‡é™åˆ¶\n",
    "            self._gini(y) > 0                                       # çº¯åº¦é™åˆ¶ï¼ˆæ‰€æœ‰æ ·æœ¬å±äºåŒä¸€ç±»æ—¶åœæ­¢ï¼‰\n",
    "        )\n",
    "        \n",
    "        if should_split:  # å¦‚æœæ»¡è¶³åˆ†å‰²æ¡ä»¶\n",
    "            idx, thr = self._best_split(X, y)  # æ‰¾åˆ°æœ€ä½³åˆ†å‰²ç‚¹\n",
    "            if idx is not None:  # å¦‚æœæ‰¾åˆ°äº†æœ‰æ•ˆçš„åˆ†å‰²ç‚¹\n",
    "                indices_left = X[:, idx] < thr  # è·å–å·¦å­æ ‘çš„æ ·æœ¬ç´¢å¼•\n",
    "                X_left, y_left = X[indices_left], y[indices_left]      # å·¦å­æ ‘æ•°æ®\n",
    "                X_right, y_right = X[~indices_left], y[~indices_left]  # å³å­æ ‘æ•°æ®\n",
    "                \n",
    "                # åªæœ‰å½“ä¸¤ä¸ªå­æ ‘éƒ½æœ‰æ ·æœ¬æ—¶æ‰è¿›è¡Œåˆ†å‰²\n",
    "                if len(y_left) > 0 and len(y_right) > 0:\n",
    "                    node.feature_index = idx  # è®¾ç½®åˆ†å‰²ç‰¹å¾\n",
    "                    node.threshold = thr      # è®¾ç½®åˆ†å‰²é˜ˆå€¼\n",
    "                    # é€’å½’æ„å»ºå·¦å³å­æ ‘\n",
    "                    node.left = self._grow_tree(X_left, y_left, depth + 1)\n",
    "                    node.right = self._grow_tree(X_right, y_right, depth + 1)\n",
    "        \n",
    "        return node  # è¿”å›å½“å‰èŠ‚ç‚¹\n",
    "        \n",
    "    def _predict(self, inputs):\n",
    "        \"\"\"é¢„æµ‹å•ä¸ªæ ·æœ¬çš„ç±»åˆ«\n",
    "        Args:\n",
    "            inputs: å•ä¸ªæ ·æœ¬çš„ç‰¹å¾å‘é‡\n",
    "        Returns:\n",
    "            é¢„æµ‹çš„ç±»åˆ«\n",
    "        \"\"\"\n",
    "        node = self.tree_  # ä»æ ¹èŠ‚ç‚¹å¼€å§‹\n",
    "        while node.left:   # å½“ä¸æ˜¯å¶èŠ‚ç‚¹æ—¶ç»§ç»­éå†\n",
    "            if inputs[node.feature_index] < node.threshold:  # æ ¹æ®ç‰¹å¾å€¼é€‰æ‹©åˆ†æ”¯\n",
    "                node = node.left   # è¿›å…¥å·¦å­æ ‘\n",
    "            else:\n",
    "                node = node.right  # è¿›å…¥å³å­æ ‘\n",
    "        return node.predicted_class  # è¿”å›å¶èŠ‚ç‚¹çš„é¢„æµ‹ç±»åˆ«\n",
    "    \n",
    "class Node:\n",
    "    \"\"\"å†³ç­–æ ‘èŠ‚ç‚¹ç±»\"\"\"\n",
    "    \n",
    "    def __init__(self, *, predicted_class):\n",
    "        \"\"\"åˆå§‹åŒ–èŠ‚ç‚¹\n",
    "        Args:\n",
    "            predicted_class: è¯¥èŠ‚ç‚¹é¢„æµ‹çš„ç±»åˆ«\n",
    "        \"\"\"\n",
    "        self.predicted_class = predicted_class  # é¢„æµ‹ç±»åˆ«\n",
    "        self.feature_index = 0                  # åˆ†å‰²ç‰¹å¾ç´¢å¼•ï¼ˆå†…éƒ¨èŠ‚ç‚¹ä½¿ç”¨ï¼‰\n",
    "        self.threshold = 0.0                    # åˆ†å‰²é˜ˆå€¼ï¼ˆå†…éƒ¨èŠ‚ç‚¹ä½¿ç”¨ï¼‰\n",
    "        self.left = None                        # å·¦å­èŠ‚ç‚¹\n",
    "        self.right = None                       # å³å­èŠ‚ç‚¹\n",
    "\n",
    "    def is_leaf_node(self):\n",
    "        \"\"\"åˆ¤æ–­æ˜¯å¦ä¸ºå¶èŠ‚ç‚¹\n",
    "        Returns:\n",
    "            Trueå¦‚æœæ˜¯å¶èŠ‚ç‚¹ï¼ŒFalseå¦åˆ™\n",
    "        \"\"\"\n",
    "        return self.left is None and self.right is None  # æ²¡æœ‰å­èŠ‚ç‚¹çš„å°±æ˜¯å¶èŠ‚ç‚¹\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed Decision Tree:\n",
      "Accuracy: 0.7\n",
      "\n",
      "Testing edge cases:\n",
      "Testing single class scenario:\n",
      "Gini for single class: 0.0\n",
      "Testing empty array:\n",
      "Gini for empty array: 0\n",
      "\n",
      "Sklearn Decision Tree:\n",
      "Accuracy: 1.0\n",
      "\n",
      "Difference in accuracy: 0.30000000000000004\n"
     ]
    }
   ],
   "source": [
    "# Test the fixed implementation\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Load the iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Test fixed implementation\n",
    "print(\"Fixed Decision Tree:\")\n",
    "tree_fixed = DecisionTree(max_depth=3)\n",
    "tree_fixed.fit(X_train, y_train)\n",
    "y_pred_fixed = tree_fixed.predict(X_test)\n",
    "accuracy_fixed = accuracy_score(y_test, y_pred_fixed)\n",
    "print(f\"Accuracy: {accuracy_fixed}\")\n",
    "\n",
    "# Test edge cases\n",
    "print(\"\\nTesting edge cases:\")\n",
    "\n",
    "# Test with single class (should not split)\n",
    "print(\"Testing single class scenario:\")\n",
    "X_single = np.array([[1, 2], [1, 2], [1, 2]])\n",
    "y_single = np.array([0, 0, 0])\n",
    "tree_single = DecisionTree(max_depth=3)\n",
    "tree_single.fit(X_single, y_single)\n",
    "print(f\"Gini for single class: {tree_single._gini(y_single)}\")\n",
    "\n",
    "# Test with empty array\n",
    "print(\"Testing empty array:\")\n",
    "print(f\"Gini for empty array: {tree_single._gini(np.array([]))}\")\n",
    "\n",
    "# Compare with sklearn's implementation\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "print(\"\\nSklearn Decision Tree:\")\n",
    "tree_sklearn = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "tree_sklearn.fit(X_train, y_train)\n",
    "y_pred_sklearn = tree_sklearn.predict(X_test)\n",
    "accuracy_sklearn = accuracy_score(y_test, y_pred_sklearn)\n",
    "print(f\"Accuracy: {accuracy_sklearn}\")\n",
    "\n",
    "print(f\"\\nDifference in accuracy: {abs(accuracy_fixed - accuracy_sklearn)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Test 1: Basic Decision Tree ===\n",
      "Dataset shape: (60, 2)\n",
      "Classes: [0 1]\n",
      "Accuracy: 1.000\n",
      "\n",
      "=== Test 2: Edge Cases ===\n",
      "Single class gini: 0.0\n",
      "Empty array gini: 0\n",
      "Perfect separation accuracy: 0.500\n",
      "\n",
      "=== Tests Completed Successfully! ===\n"
     ]
    }
   ],
   "source": [
    "# Fixed numpy-only test\n",
    "import numpy as np\n",
    "\n",
    "# Test 1: Basic functionality\n",
    "print(\"=== Test 1: Basic Decision Tree ===\")\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create simple 2D dataset: Class 0 (x < 2), Class 1 (x >= 2)\n",
    "X0 = np.random.uniform(0, 2, (30, 2))\n",
    "y0 = np.zeros(30, dtype=int)  # Ensure integer type\n",
    "X1 = np.random.uniform(2, 4, (30, 2))\n",
    "y1 = np.ones(30, dtype=int)   # Ensure integer type\n",
    "\n",
    "X = np.vstack([X0, X1])\n",
    "y = np.hstack([y0, y1])\n",
    "\n",
    "print(f\"Dataset shape: {X.shape}\")\n",
    "print(f\"Classes: {np.unique(y)}\")\n",
    "\n",
    "# Split data\n",
    "split_idx = int(0.8 * len(X))\n",
    "X_train, X_test = X[:split_idx], X[split_idx:]\n",
    "y_train, y_test = y[:split_idx], y[split_idx:]\n",
    "\n",
    "# Train and test\n",
    "tree = DecisionTree(max_depth=3)\n",
    "tree.fit(X_train, y_train)\n",
    "y_pred = tree.predict(X_test)\n",
    "accuracy = np.mean(y_pred == y_test)\n",
    "print(f\"Accuracy: {accuracy:.3f}\")\n",
    "\n",
    "# Test 2: Edge cases\n",
    "print(\"\\n=== Test 2: Edge Cases ===\")\n",
    "\n",
    "# Single class\n",
    "X_single = np.array([[1, 2], [1, 2], [1, 2]])\n",
    "y_single = np.array([0, 0, 0], dtype=int)\n",
    "tree_single = DecisionTree(max_depth=3)\n",
    "tree_single.fit(X_single, y_single)\n",
    "print(f\"Single class gini: {tree_single._gini(y_single)}\")\n",
    "\n",
    "# Empty array\n",
    "print(f\"Empty array gini: {tree_single._gini(np.array([]))}\")\n",
    "\n",
    "# Perfect separation\n",
    "X_perfect = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y_perfect = np.array([0, 1, 1, 0], dtype=int)\n",
    "tree_perfect = DecisionTree(max_depth=2)\n",
    "tree_perfect.fit(X_perfect, y_perfect)\n",
    "pred_perfect = tree_perfect.predict(X_perfect)\n",
    "acc_perfect = np.mean(pred_perfect == y_perfect)\n",
    "print(f\"Perfect separation accuracy: {acc_perfect:.3f}\")\n",
    "\n",
    "print(\"\\n=== Tests Completed Successfully! ===\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Test 1: Basic Decision Tree ===\n",
      "Dataset shape: (60, 2)\n",
      "Classes: [0. 1.]\n",
      "Accuracy: 1.000\n",
      "\n",
      "=== Test 2: Edge Cases ===\n",
      "Single class gini: 0.0\n",
      "Empty array gini: 0\n",
      "Perfect separation accuracy: 0.500\n",
      "\n",
      "=== Tests Completed ===\n"
     ]
    }
   ],
   "source": [
    "# Simple numpy-only test\n",
    "import numpy as np\n",
    "\n",
    "# Test 1: Basic functionality\n",
    "print(\"=== Test 1: Basic Decision Tree ===\")\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create simple 2D dataset: Class 0 (x < 2), Class 1 (x >= 2)\n",
    "X0 = np.random.uniform(0, 2, (30, 2))\n",
    "y0 = np.zeros(30)\n",
    "X1 = np.random.uniform(2, 4, (30, 2))\n",
    "y1 = np.ones(30)\n",
    "\n",
    "X = np.vstack([X0, X1])\n",
    "y = np.hstack([y0, y1])\n",
    "\n",
    "print(f\"Dataset shape: {X.shape}\")\n",
    "print(f\"Classes: {np.unique(y)}\")\n",
    "\n",
    "# Split data\n",
    "split_idx = int(0.8 * len(X))\n",
    "X_train, X_test = X[:split_idx], X[split_idx:]\n",
    "y_train, y_test = y[:split_idx], y[split_idx:]\n",
    "\n",
    "# Train and test\n",
    "tree = DecisionTree(max_depth=3)\n",
    "tree.fit(X_train, y_train)\n",
    "y_pred = tree.predict(X_test)\n",
    "accuracy = np.mean(y_pred == y_test)\n",
    "print(f\"Accuracy: {accuracy:.3f}\")\n",
    "\n",
    "# Test 2: Edge cases\n",
    "print(\"\\n=== Test 2: Edge Cases ===\")\n",
    "\n",
    "# Single class\n",
    "X_single = np.array([[1, 2], [1, 2], [1, 2]])\n",
    "y_single = np.array([0, 0, 0])\n",
    "tree_single = DecisionTree(max_depth=3)\n",
    "tree_single.fit(X_single, y_single)\n",
    "print(f\"Single class gini: {tree_single._gini(y_single)}\")\n",
    "\n",
    "# Empty array\n",
    "print(f\"Empty array gini: {tree_single._gini(np.array([]))}\")\n",
    "\n",
    "# Perfect separation\n",
    "X_perfect = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y_perfect = np.array([0, 1, 1, 0])\n",
    "tree_perfect = DecisionTree(max_depth=2)\n",
    "tree_perfect.fit(X_perfect, y_perfect)\n",
    "pred_perfect = tree_perfect.predict(X_perfect)\n",
    "acc_perfect = np.mean(pred_perfect == y_perfect)\n",
    "print(f\"Perfect separation accuracy: {acc_perfect:.3f}\")\n",
    "\n",
    "print(\"\\n=== Tests Completed ===\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interview Questions & Answers / é¢è¯•é—®é¢˜ä¸ç­”æ¡ˆ\n",
    "\n",
    "## ğŸ” Common Interview Questions / å¸¸è§é¢è¯•é—®é¢˜\n",
    "\n",
    "### 1. **Algorithm Understanding / ç®—æ³•ç†è§£**\n",
    "**Q: Why use Gini impurity over Entropy? / ä¸ºä»€ä¹ˆä½¿ç”¨åŸºå°¼ä¸çº¯åº¦è€Œä¸æ˜¯ç†µï¼Ÿ**\n",
    "- **A**: Gini is computationally faster (no log calculation) and gives similar results / åŸºå°¼è®¡ç®—æ›´å¿«ï¼ˆæ— éœ€å¯¹æ•°è®¡ç®—ï¼‰ä¸”ç»“æœç›¸ä¼¼\n",
    "- **Trade-off**: Gini is more sensitive to class probability changes / æƒè¡¡ï¼šåŸºå°¼å¯¹ç±»åˆ«æ¦‚ç‡å˜åŒ–æ›´æ•æ„Ÿ\n",
    "\n",
    "### 2. **Implementation Details / å®ç°ç»†èŠ‚**\n",
    "**Q: What's the time complexity of building a decision tree? / æ„å»ºå†³ç­–æ ‘çš„æ—¶é—´å¤æ‚åº¦æ˜¯ä»€ä¹ˆï¼Ÿ**\n",
    "- **A**: O(n * m * log n) where n=samples, m=features / O(n * m * log n)ï¼Œå…¶ä¸­n=æ ·æœ¬æ•°ï¼Œm=ç‰¹å¾æ•°\n",
    "- **Explanation**: For each node, we check all features (m) and all possible splits (n log n) / è§£é‡Šï¼šå¯¹æ¯ä¸ªèŠ‚ç‚¹ï¼Œæˆ‘ä»¬æ£€æŸ¥æ‰€æœ‰ç‰¹å¾(m)å’Œæ‰€æœ‰å¯èƒ½åˆ†å‰²(n log n)\n",
    "\n",
    "### 3. **Edge Cases / è¾¹ç•Œæƒ…å†µ**\n",
    "**Q: How do you handle categorical features? / å¦‚ä½•å¤„ç†åˆ†ç±»ç‰¹å¾ï¼Ÿ**\n",
    "- **A**: One-hot encoding or target encoding / ç‹¬çƒ­ç¼–ç æˆ–ç›®æ ‡ç¼–ç \n",
    "- **Alternative**: Modify splitting criteria for categorical splits / æ›¿ä»£æ–¹æ¡ˆï¼šä¿®æ”¹åˆ†ç±»åˆ†å‰²çš„åˆ†å‰²æ ‡å‡†\n",
    "\n",
    "### 4. **Optimization / ä¼˜åŒ–**\n",
    "**Q: How would you optimize this implementation? / å¦‚ä½•ä¼˜åŒ–è¿™ä¸ªå®ç°ï¼Ÿ**\n",
    "- **A**: \n",
    "  - Use vectorized operations / ä½¿ç”¨å‘é‡åŒ–æ“ä½œ\n",
    "  - Implement early stopping / å®ç°æ—©åœ\n",
    "  - Add feature importance / æ·»åŠ ç‰¹å¾é‡è¦æ€§\n",
    "  - Support parallel processing / æ”¯æŒå¹¶è¡Œå¤„ç†\n",
    "\n",
    "### 5. **Real-world Considerations / å®é™…è€ƒè™‘**\n",
    "**Q: What are the limitations of decision trees? / å†³ç­–æ ‘çš„å±€é™æ€§æ˜¯ä»€ä¹ˆï¼Ÿ**\n",
    "- **A**:\n",
    "  - Overfitting with deep trees / æ·±åº¦æ ‘è¿‡æ‹Ÿåˆ\n",
    "  - Poor performance on XOR-like problems / åœ¨XORç±»é—®é¢˜ä¸Šè¡¨ç°å·®\n",
    "  - Instability with small data changes / æ•°æ®å°å˜åŒ–æ—¶ä¸ç¨³å®š\n",
    "  - Bias towards features with more levels / åå‘æ›´å¤šçº§åˆ«çš„ç‰¹å¾\n",
    "\n",
    "## ğŸ¯ Key Takeaways / å…³é”®è¦ç‚¹\n",
    "\n",
    "1. **Always clarify requirements first** / æ€»æ˜¯å…ˆæ¾„æ¸…éœ€æ±‚\n",
    "2. **Handle edge cases explicitly** / æ˜ç¡®å¤„ç†è¾¹ç•Œæƒ…å†µ  \n",
    "3. **Understand computational complexity** / ç†è§£è®¡ç®—å¤æ‚åº¦\n",
    "4. **Know when NOT to use decision trees** / çŸ¥é“ä½•æ—¶ä¸ä½¿ç”¨å†³ç­–æ ‘\n",
    "5. **Be ready to discuss ensemble methods** / å‡†å¤‡å¥½è®¨è®ºé›†æˆæ–¹æ³•\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the decision tree\n",
    "tree = DecisionTree(max_depth=3)\n",
    "tree.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = tree.predict(X_test)\n",
    "\n",
    "# Compute the accuracy of the predictions\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cyclegan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
