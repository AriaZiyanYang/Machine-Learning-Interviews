{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Implementation / 决策树实现\n",
    "\n",
    "## Algorithm Overview / 算法概述\n",
    "\n",
    "A decision tree is a type of machine learning algorithm used for classification and regression tasks. It consists of a tree-like structure where each internal node represents a feature or attribute, each branch represents a decision based on that feature, and each leaf node represents a predicted output.\n",
    "\n",
    "决策树是一种用于分类和回归任务的机器学习算法。它由树状结构组成，其中每个内部节点代表一个特征或属性，每个分支代表基于该特征的决策，每个叶节点代表预测输出。\n",
    "\n",
    "## Training Process / 训练过程\n",
    "\n",
    "To **train** a decision tree, the algorithm uses a dataset with labeled examples to create the tree structure. It starts with the root node, which includes all the examples, and selects the feature that provides the most information gain to split the data into two subsets. It then repeats this process for each subset until it reaches a stopping criterion, such as a maximum tree depth or minimum number of examples in a leaf node.\n",
    "\n",
    "要**训练**决策树，算法使用带有标签示例的数据集来创建树结构。它从包含所有示例的根节点开始，选择提供最大信息增益的特征将数据分割成两个子集。然后对每个子集重复此过程，直到达到停止标准，如最大树深度或叶节点中的最小示例数。\n",
    "\n",
    "## Prediction Process / 预测过程\n",
    "\n",
    "Once the decision tree is trained, it can be used to **predict** the output for new, unseen examples. To make a prediction, the algorithm starts at the root node and follows the branches based on the values of the input features until it reaches a leaf node. The predicted output for that example is the value associated with the leaf node.\n",
    "\n",
    "一旦决策树训练完成，就可以用来**预测**新的、未见过的示例的输出。要进行预测，算法从根节点开始，根据输入特征的值沿着分支，直到到达叶节点。该示例的预测输出是与叶节点关联的值。\n",
    "\n",
    "## Advantages and Limitations / 优缺点\n",
    "\n",
    "**Advantages / 优点:**\n",
    "- Easy to interpret and visualize / 易于解释和可视化\n",
    "- Handle both numerical and categorical data / 处理数值和分类数据\n",
    "- Handle missing values / 处理缺失值\n",
    "\n",
    "**Limitations / 局限性:**\n",
    "- Can suffer from overfitting / 可能过拟合\n",
    "- May not perform well with highly imbalanced datasets / 在高度不平衡的数据集上可能表现不佳\n",
    "- Not suitable for highly nonlinear relationships / 不适合高度非线性关系\n",
    "\n",
    "## Interview Clarification Questions / 面试澄清问题\n",
    "\n",
    "**🔍 Key Questions to Ask:**\n",
    "1. **Data Type**: Numerical only or categorical too? / 数据类型：仅数值还是包括分类？\n",
    "2. **Splitting Criteria**: Gini, Entropy, or MSE? / 分割标准：基尼、熵还是均方误差？\n",
    "3. **Stopping Conditions**: Max depth, min samples, min impurity? / 停止条件：最大深度、最小样本、最小不纯度？\n",
    "4. **Missing Values**: How to handle? / 缺失值：如何处理？\n",
    "5. **Multi-class**: Binary or multi-class classification? / 多类：二分类还是多分类？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  # Import numpy for numerical computations / 导入numpy库用于数值计算\n",
    "\n",
    "class DecisionTree:\n",
    "    \"\"\"Decision Tree Classifier Implementation / 决策树分类器实现\n",
    "    \n",
    "    🔍 Interview Clarification Questions / 面试澄清问题:\n",
    "    1. Should we support both classification and regression? / 是否支持分类和回归？\n",
    "    2. What splitting criteria to use? (Gini, Entropy, MSE) / 使用什么分割标准？\n",
    "    3. How to handle categorical features? / 如何处理分类特征？\n",
    "    4. What stopping conditions? / 什么停止条件？\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, max_depth=None):\n",
    "        \"\"\"Initialize Decision Tree / 初始化决策树\n",
    "        \n",
    "        Args / 参数:\n",
    "            max_depth: Maximum tree depth, None means unlimited / 树的最大深度，None表示无限制\n",
    "            \n",
    "        🔍 Interview Questions / 面试问题:\n",
    "        - Why limit depth? (Prevent overfitting) / 为什么限制深度？（防止过拟合）\n",
    "        - What other parameters might be useful? / 其他有用参数？\n",
    "        \"\"\"\n",
    "        self.max_depth = max_depth  # Store max depth parameter / 存储最大深度参数\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Train the decision tree / 训练决策树\n",
    "        \n",
    "        Args / 参数:\n",
    "            X: Feature matrix (n_samples, n_features) / 特征矩阵\n",
    "            y: Label vector (n_samples,) / 标签向量\n",
    "            \n",
    "        🔍 Interview Questions / 面试问题:\n",
    "        - Should we validate input data? / 是否验证输入数据？\n",
    "        - How to handle different data types? / 如何处理不同数据类型？\n",
    "        - What if features have different scales? / 如果特征尺度不同怎么办？\n",
    "        \"\"\"\n",
    "        self.n_classes_ = len(np.unique(y))  # Count number of classes / 计算类别数量\n",
    "        self.n_features_ = X.shape[1]        # Get number of features / 获取特征数量\n",
    "        self.tree_ = self._grow_tree(X, y)   # Recursively build decision tree / 递归构建决策树\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict classes for new samples / 预测新样本的类别\n",
    "        \n",
    "        Args / 参数:\n",
    "            X: Feature matrix (n_samples, n_features) / 特征矩阵\n",
    "            \n",
    "        Returns / 返回:\n",
    "            List of predicted classes / 预测的类别列表\n",
    "            \n",
    "        🔍 Interview Questions / 面试问题:\n",
    "        - Should we return probabilities too? / 是否也返回概率？\n",
    "        - How to handle batch vs single prediction? / 如何处理批量vs单个预测？\n",
    "        - What if tree is not trained? / 如果树未训练怎么办？\n",
    "        \"\"\"\n",
    "        return [self._predict(inputs) for inputs in X]  # Predict each sample / 对每个样本进行预测\n",
    "        \n",
    "    def _gini(self, y):\n",
    "        \"\"\"Calculate Gini impurity / 计算基尼不纯度\n",
    "        \n",
    "        Args / 参数:\n",
    "            y: Label array / 标签数组\n",
    "            \n",
    "        Returns / 返回:\n",
    "            Gini impurity value (0-1, 0 means perfectly pure) / 基尼不纯度值 (0-1之间，0表示完全纯净)\n",
    "            \n",
    "        🔍 Interview Questions / 面试问题:\n",
    "        - Why Gini over Entropy? / 为什么选择基尼而不是熵？\n",
    "        - How does Gini behave with imbalanced classes? / 基尼在不平衡类别中如何表现？\n",
    "        - What's the computational complexity? / 计算复杂度是什么？\n",
    "        \"\"\"\n",
    "        if len(y) == 0:  # If array is empty, return 0 / 如果数组为空，返回0\n",
    "            return 0\n",
    "        _, counts = np.unique(y, return_counts=True)  # Count each class / 统计每个类别的数量\n",
    "        probabilities = counts / len(y)               # Calculate class probabilities / 计算每个类别的概率\n",
    "        return 1 - np.sum(probabilities ** 2)          # Gini formula: 1 - Σ(p_i)² / 基尼不纯度公式\n",
    "        \n",
    "    def _best_split(self, X, y):\n",
    "        \"\"\"Find the best split point / 找到最佳分割点\n",
    "        \n",
    "        Args / 参数:\n",
    "            X: Feature matrix / 特征矩阵\n",
    "            y: Label array / 标签数组\n",
    "            \n",
    "        Returns / 返回:\n",
    "            (best_feature_index, best_threshold) or (None, None) / 最佳特征索引和阈值或None\n",
    "            \n",
    "        🔍 Interview Questions / 面试问题:\n",
    "        - How to handle continuous vs discrete features? / 如何处理连续vs离散特征？\n",
    "        - What's the time complexity? / 时间复杂度是什么？\n",
    "        - How to handle ties in impurity? / 如何处理不纯度相等的情况？\n",
    "        \"\"\"\n",
    "        m = y.size  # Get number of samples / 获取样本数量\n",
    "        if m <= 1:  # If <=1 samples, cannot split / 如果样本数<=1，无法分割\n",
    "            return None, None\n",
    "        \n",
    "        # Calculate parent node Gini impurity / 计算父节点的基尼不纯度\n",
    "        parent_gini = self._gini(y)\n",
    "        best_gini = parent_gini  # Initialize best Gini / 初始化最佳基尼不纯度\n",
    "        best_idx, best_thr = None, None  # Initialize best feature and threshold / 初始化最佳特征索引和阈值\n",
    "        \n",
    "        # Iterate through all features / 遍历所有特征\n",
    "        for idx in range(self.n_features_):\n",
    "            # Sort by feature values while keeping corresponding labels / 按特征值排序，同时保持对应的标签\n",
    "            thresholds, classes = zip(*sorted(zip(X[:, idx], y)))\n",
    "            num_left = [0] * self.n_classes_  # Left subtree class counts / 左子树的类别计数\n",
    "            num_right = [np.sum(y == c) for c in range(self.n_classes_)]  # Right subtree class counts / 右子树的类别计数\n",
    "            \n",
    "            # Try all possible split points / 尝试所有可能的分割点\n",
    "            for i in range(1, m):\n",
    "                c = int(classes[i - 1])  # Get current sample class, convert to int / 获取当前样本的类别，转换为整数\n",
    "                num_left[c] += 1         # Move sample to left subtree / 将样本移到左子树\n",
    "                num_right[c] -= 1       # Remove sample from right subtree / 从右子树移除样本\n",
    "                \n",
    "                # Skip if thresholds are the same (avoid duplicate calculation) / 如果阈值相同，跳过（避免重复计算）\n",
    "                if thresholds[i] == thresholds[i - 1]:\n",
    "                    continue\n",
    "                    \n",
    "                # Calculate weighted Gini impurity / 计算加权基尼不纯度\n",
    "                gini_left = self._gini(y[:i])    # Left subtree Gini / 左子树的基尼不纯度\n",
    "                gini_right = self._gini(y[i:])   # Right subtree Gini / 右子树的基尼不纯度\n",
    "                weighted_gini = (i * gini_left + (m - i) * gini_right) / m  # Weighted average / 加权平均\n",
    "                \n",
    "                # If found better split, update best split / 如果找到更好的分割点，更新最佳分割\n",
    "                if weighted_gini < best_gini:\n",
    "                    best_gini = weighted_gini\n",
    "                    best_idx = idx\n",
    "                    best_thr = (thresholds[i] + thresholds[i - 1]) / 2  # Use midpoint as threshold / 取中点作为阈值\n",
    "        \n",
    "        return best_idx, best_thr  # Return best split feature and threshold / 返回最佳分割特征和阈值\n",
    "        \n",
    "    def _grow_tree(self, X, y, depth=0):\n",
    "        \"\"\"递归构建决策树\n",
    "        Args:\n",
    "            X: 特征矩阵\n",
    "            y: 标签数组\n",
    "            depth: 当前深度\n",
    "        Returns:\n",
    "            树的根节点\n",
    "        \"\"\"\n",
    "        # 计算每个类别的样本数量\n",
    "        num_samples_per_class = [np.sum(y == i) for i in range(self.n_classes_)]\n",
    "        predicted_class = np.argmax(num_samples_per_class)  # 选择样本最多的类别作为预测\n",
    "        node = Node(predicted_class=predicted_class)        # 创建新节点\n",
    "        \n",
    "        # 检查是否应该继续分割\n",
    "        should_split = (\n",
    "            (self.max_depth is None or depth < self.max_depth) and  # 深度限制\n",
    "            len(y) > 1 and                                          # 样本数量限制\n",
    "            self._gini(y) > 0                                       # 纯度限制（所有样本属于同一类时停止）\n",
    "        )\n",
    "        \n",
    "        if should_split:  # 如果满足分割条件\n",
    "            idx, thr = self._best_split(X, y)  # 找到最佳分割点\n",
    "            if idx is not None:  # 如果找到了有效的分割点\n",
    "                indices_left = X[:, idx] < thr  # 获取左子树的样本索引\n",
    "                X_left, y_left = X[indices_left], y[indices_left]      # 左子树数据\n",
    "                X_right, y_right = X[~indices_left], y[~indices_left]  # 右子树数据\n",
    "                \n",
    "                # 只有当两个子树都有样本时才进行分割\n",
    "                if len(y_left) > 0 and len(y_right) > 0:\n",
    "                    node.feature_index = idx  # 设置分割特征\n",
    "                    node.threshold = thr      # 设置分割阈值\n",
    "                    # 递归构建左右子树\n",
    "                    node.left = self._grow_tree(X_left, y_left, depth + 1)\n",
    "                    node.right = self._grow_tree(X_right, y_right, depth + 1)\n",
    "        \n",
    "        return node  # 返回当前节点\n",
    "        \n",
    "    def _predict(self, inputs):\n",
    "        \"\"\"预测单个样本的类别\n",
    "        Args:\n",
    "            inputs: 单个样本的特征向量\n",
    "        Returns:\n",
    "            预测的类别\n",
    "        \"\"\"\n",
    "        node = self.tree_  # 从根节点开始\n",
    "        while node.left:   # 当不是叶节点时继续遍历\n",
    "            if inputs[node.feature_index] < node.threshold:  # 根据特征值选择分支\n",
    "                node = node.left   # 进入左子树\n",
    "            else:\n",
    "                node = node.right  # 进入右子树\n",
    "        return node.predicted_class  # 返回叶节点的预测类别\n",
    "    \n",
    "class Node:\n",
    "    \"\"\"决策树节点类\"\"\"\n",
    "    \n",
    "    def __init__(self, *, predicted_class):\n",
    "        \"\"\"初始化节点\n",
    "        Args:\n",
    "            predicted_class: 该节点预测的类别\n",
    "        \"\"\"\n",
    "        self.predicted_class = predicted_class  # 预测类别\n",
    "        self.feature_index = 0                  # 分割特征索引（内部节点使用）\n",
    "        self.threshold = 0.0                    # 分割阈值（内部节点使用）\n",
    "        self.left = None                        # 左子节点\n",
    "        self.right = None                       # 右子节点\n",
    "\n",
    "    def is_leaf_node(self):\n",
    "        \"\"\"判断是否为叶节点\n",
    "        Returns:\n",
    "            True如果是叶节点，False否则\n",
    "        \"\"\"\n",
    "        return self.left is None and self.right is None  # 没有子节点的就是叶节点\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed Decision Tree:\n",
      "Accuracy: 0.7\n",
      "\n",
      "Testing edge cases:\n",
      "Testing single class scenario:\n",
      "Gini for single class: 0.0\n",
      "Testing empty array:\n",
      "Gini for empty array: 0\n",
      "\n",
      "Sklearn Decision Tree:\n",
      "Accuracy: 1.0\n",
      "\n",
      "Difference in accuracy: 0.30000000000000004\n"
     ]
    }
   ],
   "source": [
    "# Test the fixed implementation\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Load the iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Test fixed implementation\n",
    "print(\"Fixed Decision Tree:\")\n",
    "tree_fixed = DecisionTree(max_depth=3)\n",
    "tree_fixed.fit(X_train, y_train)\n",
    "y_pred_fixed = tree_fixed.predict(X_test)\n",
    "accuracy_fixed = accuracy_score(y_test, y_pred_fixed)\n",
    "print(f\"Accuracy: {accuracy_fixed}\")\n",
    "\n",
    "# Test edge cases\n",
    "print(\"\\nTesting edge cases:\")\n",
    "\n",
    "# Test with single class (should not split)\n",
    "print(\"Testing single class scenario:\")\n",
    "X_single = np.array([[1, 2], [1, 2], [1, 2]])\n",
    "y_single = np.array([0, 0, 0])\n",
    "tree_single = DecisionTree(max_depth=3)\n",
    "tree_single.fit(X_single, y_single)\n",
    "print(f\"Gini for single class: {tree_single._gini(y_single)}\")\n",
    "\n",
    "# Test with empty array\n",
    "print(\"Testing empty array:\")\n",
    "print(f\"Gini for empty array: {tree_single._gini(np.array([]))}\")\n",
    "\n",
    "# Compare with sklearn's implementation\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "print(\"\\nSklearn Decision Tree:\")\n",
    "tree_sklearn = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "tree_sklearn.fit(X_train, y_train)\n",
    "y_pred_sklearn = tree_sklearn.predict(X_test)\n",
    "accuracy_sklearn = accuracy_score(y_test, y_pred_sklearn)\n",
    "print(f\"Accuracy: {accuracy_sklearn}\")\n",
    "\n",
    "print(f\"\\nDifference in accuracy: {abs(accuracy_fixed - accuracy_sklearn)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Test 1: Basic Decision Tree ===\n",
      "Dataset shape: (60, 2)\n",
      "Classes: [0 1]\n",
      "Accuracy: 1.000\n",
      "\n",
      "=== Test 2: Edge Cases ===\n",
      "Single class gini: 0.0\n",
      "Empty array gini: 0\n",
      "Perfect separation accuracy: 0.500\n",
      "\n",
      "=== Tests Completed Successfully! ===\n"
     ]
    }
   ],
   "source": [
    "# Fixed numpy-only test\n",
    "import numpy as np\n",
    "\n",
    "# Test 1: Basic functionality\n",
    "print(\"=== Test 1: Basic Decision Tree ===\")\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create simple 2D dataset: Class 0 (x < 2), Class 1 (x >= 2)\n",
    "X0 = np.random.uniform(0, 2, (30, 2))\n",
    "y0 = np.zeros(30, dtype=int)  # Ensure integer type\n",
    "X1 = np.random.uniform(2, 4, (30, 2))\n",
    "y1 = np.ones(30, dtype=int)   # Ensure integer type\n",
    "\n",
    "X = np.vstack([X0, X1])\n",
    "y = np.hstack([y0, y1])\n",
    "\n",
    "print(f\"Dataset shape: {X.shape}\")\n",
    "print(f\"Classes: {np.unique(y)}\")\n",
    "\n",
    "# Split data\n",
    "split_idx = int(0.8 * len(X))\n",
    "X_train, X_test = X[:split_idx], X[split_idx:]\n",
    "y_train, y_test = y[:split_idx], y[split_idx:]\n",
    "\n",
    "# Train and test\n",
    "tree = DecisionTree(max_depth=3)\n",
    "tree.fit(X_train, y_train)\n",
    "y_pred = tree.predict(X_test)\n",
    "accuracy = np.mean(y_pred == y_test)\n",
    "print(f\"Accuracy: {accuracy:.3f}\")\n",
    "\n",
    "# Test 2: Edge cases\n",
    "print(\"\\n=== Test 2: Edge Cases ===\")\n",
    "\n",
    "# Single class\n",
    "X_single = np.array([[1, 2], [1, 2], [1, 2]])\n",
    "y_single = np.array([0, 0, 0], dtype=int)\n",
    "tree_single = DecisionTree(max_depth=3)\n",
    "tree_single.fit(X_single, y_single)\n",
    "print(f\"Single class gini: {tree_single._gini(y_single)}\")\n",
    "\n",
    "# Empty array\n",
    "print(f\"Empty array gini: {tree_single._gini(np.array([]))}\")\n",
    "\n",
    "# Perfect separation\n",
    "X_perfect = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y_perfect = np.array([0, 1, 1, 0], dtype=int)\n",
    "tree_perfect = DecisionTree(max_depth=2)\n",
    "tree_perfect.fit(X_perfect, y_perfect)\n",
    "pred_perfect = tree_perfect.predict(X_perfect)\n",
    "acc_perfect = np.mean(pred_perfect == y_perfect)\n",
    "print(f\"Perfect separation accuracy: {acc_perfect:.3f}\")\n",
    "\n",
    "print(\"\\n=== Tests Completed Successfully! ===\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Test 1: Basic Decision Tree ===\n",
      "Dataset shape: (60, 2)\n",
      "Classes: [0. 1.]\n",
      "Accuracy: 1.000\n",
      "\n",
      "=== Test 2: Edge Cases ===\n",
      "Single class gini: 0.0\n",
      "Empty array gini: 0\n",
      "Perfect separation accuracy: 0.500\n",
      "\n",
      "=== Tests Completed ===\n"
     ]
    }
   ],
   "source": [
    "# Simple numpy-only test\n",
    "import numpy as np\n",
    "\n",
    "# Test 1: Basic functionality\n",
    "print(\"=== Test 1: Basic Decision Tree ===\")\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create simple 2D dataset: Class 0 (x < 2), Class 1 (x >= 2)\n",
    "X0 = np.random.uniform(0, 2, (30, 2))\n",
    "y0 = np.zeros(30)\n",
    "X1 = np.random.uniform(2, 4, (30, 2))\n",
    "y1 = np.ones(30)\n",
    "\n",
    "X = np.vstack([X0, X1])\n",
    "y = np.hstack([y0, y1])\n",
    "\n",
    "print(f\"Dataset shape: {X.shape}\")\n",
    "print(f\"Classes: {np.unique(y)}\")\n",
    "\n",
    "# Split data\n",
    "split_idx = int(0.8 * len(X))\n",
    "X_train, X_test = X[:split_idx], X[split_idx:]\n",
    "y_train, y_test = y[:split_idx], y[split_idx:]\n",
    "\n",
    "# Train and test\n",
    "tree = DecisionTree(max_depth=3)\n",
    "tree.fit(X_train, y_train)\n",
    "y_pred = tree.predict(X_test)\n",
    "accuracy = np.mean(y_pred == y_test)\n",
    "print(f\"Accuracy: {accuracy:.3f}\")\n",
    "\n",
    "# Test 2: Edge cases\n",
    "print(\"\\n=== Test 2: Edge Cases ===\")\n",
    "\n",
    "# Single class\n",
    "X_single = np.array([[1, 2], [1, 2], [1, 2]])\n",
    "y_single = np.array([0, 0, 0])\n",
    "tree_single = DecisionTree(max_depth=3)\n",
    "tree_single.fit(X_single, y_single)\n",
    "print(f\"Single class gini: {tree_single._gini(y_single)}\")\n",
    "\n",
    "# Empty array\n",
    "print(f\"Empty array gini: {tree_single._gini(np.array([]))}\")\n",
    "\n",
    "# Perfect separation\n",
    "X_perfect = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y_perfect = np.array([0, 1, 1, 0])\n",
    "tree_perfect = DecisionTree(max_depth=2)\n",
    "tree_perfect.fit(X_perfect, y_perfect)\n",
    "pred_perfect = tree_perfect.predict(X_perfect)\n",
    "acc_perfect = np.mean(pred_perfect == y_perfect)\n",
    "print(f\"Perfect separation accuracy: {acc_perfect:.3f}\")\n",
    "\n",
    "print(\"\\n=== Tests Completed ===\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interview Questions & Answers / 面试问题与答案\n",
    "\n",
    "## 🔍 Common Interview Questions / 常见面试问题\n",
    "\n",
    "### 1. **Algorithm Understanding / 算法理解**\n",
    "**Q: Why use Gini impurity over Entropy? / 为什么使用基尼不纯度而不是熵？**\n",
    "- **A**: Gini is computationally faster (no log calculation) and gives similar results / 基尼计算更快（无需对数计算）且结果相似\n",
    "- **Trade-off**: Gini is more sensitive to class probability changes / 权衡：基尼对类别概率变化更敏感\n",
    "\n",
    "### 2. **Implementation Details / 实现细节**\n",
    "**Q: What's the time complexity of building a decision tree? / 构建决策树的时间复杂度是什么？**\n",
    "- **A**: O(n * m * log n) where n=samples, m=features / O(n * m * log n)，其中n=样本数，m=特征数\n",
    "- **Explanation**: For each node, we check all features (m) and all possible splits (n log n) / 解释：对每个节点，我们检查所有特征(m)和所有可能分割(n log n)\n",
    "\n",
    "### 3. **Edge Cases / 边界情况**\n",
    "**Q: How do you handle categorical features? / 如何处理分类特征？**\n",
    "- **A**: One-hot encoding or target encoding / 独热编码或目标编码\n",
    "- **Alternative**: Modify splitting criteria for categorical splits / 替代方案：修改分类分割的分割标准\n",
    "\n",
    "### 4. **Optimization / 优化**\n",
    "**Q: How would you optimize this implementation? / 如何优化这个实现？**\n",
    "- **A**: \n",
    "  - Use vectorized operations / 使用向量化操作\n",
    "  - Implement early stopping / 实现早停\n",
    "  - Add feature importance / 添加特征重要性\n",
    "  - Support parallel processing / 支持并行处理\n",
    "\n",
    "### 5. **Real-world Considerations / 实际考虑**\n",
    "**Q: What are the limitations of decision trees? / 决策树的局限性是什么？**\n",
    "- **A**:\n",
    "  - Overfitting with deep trees / 深度树过拟合\n",
    "  - Poor performance on XOR-like problems / 在XOR类问题上表现差\n",
    "  - Instability with small data changes / 数据小变化时不稳定\n",
    "  - Bias towards features with more levels / 偏向更多级别的特征\n",
    "\n",
    "## 🎯 Key Takeaways / 关键要点\n",
    "\n",
    "1. **Always clarify requirements first** / 总是先澄清需求\n",
    "2. **Handle edge cases explicitly** / 明确处理边界情况  \n",
    "3. **Understand computational complexity** / 理解计算复杂度\n",
    "4. **Know when NOT to use decision trees** / 知道何时不使用决策树\n",
    "5. **Be ready to discuss ensemble methods** / 准备好讨论集成方法\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the decision tree\n",
    "tree = DecisionTree(max_depth=3)\n",
    "tree.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = tree.predict(X_test)\n",
    "\n",
    "# Compute the accuracy of the predictions\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cyclegan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
